{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf072104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, confusion_matrix, roc_curve, accuracy_score\n",
    "import json\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from lightgbm import record_evaluation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287139ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_feature_names(df):\n",
    "    \"\"\"\n",
    "    Fully compatible function to remove parentheses\n",
    "    \"\"\"\n",
    "    # Define cleaning function - uses regex to remove all types of parentheses\n",
    "    def clean_name(name):\n",
    "        # Use regex to remove all types of parentheses\n",
    "        cleaned = name.replace('(', '').replace(')', '').replace(',', '')\n",
    "        return cleaned\n",
    "    \n",
    "    # Apply cleaning function to all column names\n",
    "    new_columns = [clean_name(col) for col in df.columns]\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "def add_time_features(\n",
    "    df: pd.DataFrame,\n",
    "    id_col: str = 'Athlete ID',\n",
    "    date_col: str = 'Date',\n",
    "    window_sizes: list = [7, 14, 28]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate rolling statistics for each athlete by date: mean, std, max, min, injury_rate\n",
    "    window_sizes: Rolling window sizes in days or weeks\n",
    "    \"\"\"\n",
    "    df = df.sort_values([id_col, date_col]).reset_index(drop=True)\n",
    "    out = df[[id_col, date_col]].copy()\n",
    "    # Select numerical columns\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.drop(['injury'], errors='ignore')\n",
    "\n",
    "    for w in window_sizes:\n",
    "        # Unit is days (for weekly level, convert to week sequence first)\n",
    "        rolled = (\n",
    "            df\n",
    "            .groupby(id_col)[num_cols]\n",
    "            .rolling(window=w, min_periods=1, closed='left')  # Use only past data\n",
    "            .agg(['mean', 'std', 'max', 'min'])\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        # Rename columns: col_mean_7, col_std_7...\n",
    "        rolled.columns = [f\"{col}_{stat}_{w}d\" for col, stat in rolled.columns]\n",
    "        out = pd.concat([out, rolled], axis=1)\n",
    "\n",
    "        # Injury rate: mean of 'injury' in past window\n",
    "        inj_rate = (\n",
    "            df\n",
    "            .groupby(id_col)['injury']\n",
    "            .rolling(window=w, min_periods=1, closed='left')  # Use only past data\n",
    "            .mean()\n",
    "            .reset_index(level=0, drop=True)\n",
    "            .rename(f\"injury_rate_{w}d\")\n",
    "        )\n",
    "        out[f\"injury_rate_{w}d\"] = inj_rate\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def data_processing_pipeline(\n",
    "    week_data: pd.DataFrame,\n",
    "    day_data: pd.DataFrame,\n",
    "    level: str = 'day'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Data processing pipeline: Uses rolling statistics and lag features\n",
    "    Returns DataFrame with time features, lag features, and labels\n",
    "    \"\"\"\n",
    "    # 1. Select level\n",
    "    if level == 'day':\n",
    "        base = day_data.copy()\n",
    "    elif level == 'week':\n",
    "        base = week_data.copy()\n",
    "    else:\n",
    "        raise ValueError(\"level must be 'day' or 'week'\")\n",
    "\n",
    "    # Check required columns\n",
    "    for col in ['Athlete ID', 'Date', 'injury']:\n",
    "        if col not in base.columns:\n",
    "            raise KeyError(f\"Missing column {col} in {level}_data\")\n",
    "\n",
    "    # Convert time column to datetime: Assuming Date is day number (e.g., 1, 2, ..., 2673)\n",
    "    start_date = pd.to_datetime('2019-01-01')  # Customizable start date\n",
    "    base['Date'] = start_date + pd.to_timedelta(base['Date'], unit='D')\n",
    "\n",
    "    base = base.sort_values(['Athlete ID', 'Date'])\n",
    "    if level == 'week':\n",
    "        base = clean_feature_names(base)\n",
    "    \n",
    "    # 1. Add rolling window statistical features\n",
    "    time_feats = add_time_features(base)\n",
    "\n",
    "\n",
    "    # 2. Add injury label\n",
    "    time_feats['injury'] = base['injury'].values\n",
    "    time_feats['Athlete ID'] = base['Athlete ID'].values\n",
    "    time_feats['Date'] = base['Date'].values\n",
    "    \n",
    "    # 3. Remove ID and date columns\n",
    "    X = time_feats.drop(columns=['Athlete ID','Date','injury'])\n",
    "\n",
    "    # 4. Low variance filtering\n",
    "    selector = VarianceThreshold(threshold=0.001)\n",
    "    X_high_var = selector.fit_transform(X)\n",
    "    cols_high = selector.get_feature_names_out(X.columns)\n",
    "\n",
    "    # 5. Missing value imputation (median)\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = imputer.fit_transform(X_high_var)\n",
    "\n",
    "    processed = pd.DataFrame(X_imputed, columns=cols_high)\n",
    "    processed['injury'] = time_feats['injury'].values\n",
    "    processed['Athlete ID'] = time_feats['Athlete ID'].values\n",
    "    processed['Date'] = time_feats['Date'].values\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc300e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve and save to a file\n",
    "def plotROC(val_fpr, val_tpr, val_auc, test_fpr, test_tpr, test_auc, filename):\n",
    "    linewidth = 2\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(val_fpr, val_tpr, color='darkorange', lw=linewidth, label=f'Validation (AUC = {val_auc:.4f})')\n",
    "    plt.plot(test_fpr, test_tpr, color='darkgreen', lw=linewidth, label=f'Test (AUC = {test_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=linewidth, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def determine_best_threshold(y_true, y_prob, target_recall=0.70):\n",
    "    \"\"\"Select threshold based on target recall rate\"\"\"\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    best_threshold = 0.5\n",
    "    best_precision = 0.0  # Add initialization\n",
    "    highest_recall = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        \n",
    "        # Select threshold with highest precision when reaching target recall\n",
    "        if recall >= target_recall:\n",
    "            precision = precision_score(y_true, y_pred)\n",
    "            if precision > best_precision:\n",
    "                best_precision = precision\n",
    "                best_threshold = threshold\n",
    "        # Record highest recall as fallback\n",
    "        elif recall > highest_recall:\n",
    "            highest_recall = recall\n",
    "            fallback_threshold = threshold\n",
    "    \n",
    "    # Use highest recall if target recall cannot be reached\n",
    "    if best_threshold == 0.5:\n",
    "        return fallback_threshold\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_prob, threshold):\n",
    "    \"\"\"Calculate AUC, F1, and injury risk score\"\"\"\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auccuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate injury risk score (based on confusion matrix)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    injury_risk_score = tp / (tp + fn)  # Recall/sensitivity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'f1': f1,\n",
    "        'injury_risk_score': injury_risk_score,\n",
    "        'accuracy': auccuracy,\n",
    "        'precision': precision,\n",
    "        'specificity': specificity,\n",
    "        'confusion_matrix': {\n",
    "            'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp\n",
    "        }\n",
    "    }\n",
    "\n",
    "def evaluate_and_plot_model(model_name, y_val, y_val_prob, y_test, y_test_prob, best_thresh, file_prefix):\n",
    "    # ROC curve\n",
    "    val_fpr, val_tpr, _ = roc_curve(y_val, y_val_prob)\n",
    "    test_fpr, test_tpr, _ = roc_curve(y_test, y_test_prob)\n",
    "    val_auc = roc_auc_score(y_val, y_val_prob)\n",
    "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "    # Plot\n",
    "    plotROC(val_fpr, val_tpr, val_auc, test_fpr, test_tpr, test_auc, f'results/{file_prefix}_roc.png')\n",
    "    # Calculate metrics\n",
    "    metrics_test = calculate_metrics(y_test, y_test_prob, best_thresh)\n",
    "    metrics_val = calculate_metrics(y_val, y_val_prob, best_thresh)\n",
    "\n",
    "    # Output results\n",
    "    print(f\"\\n==== {model_name} Validation Set Performance ====\")\n",
    "    print(f\"AUC: {metrics_val['auc']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics_val['f1']:.4f}\")\n",
    "    print(f\"Precision: {metrics_val['precision']:.4f}\")\n",
    "    print(f\"Accuracy: {metrics_val['accuracy']:.4f}\")\n",
    "    print(f\"Specificity: {metrics_val['specificity']:.4f}\")\n",
    "    print(f\"Injury risk score(Recall): {metrics_val['injury_risk_score']:.4f}\")\n",
    "    print(f\"confusion_matrix: TN={metrics_val['confusion_matrix']['tn']}, FP={metrics_val['confusion_matrix']['fp']}, \"\n",
    "          f\"FN={metrics_val['confusion_matrix']['fn']}, TP={metrics_val['confusion_matrix']['tp']}\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n==== {model_name} Test Set Performance ====\")\n",
    "    print(f\"AUC: {metrics_test['auc']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics_test['f1']:.4f}\")\n",
    "    print(f\"Precision: {metrics_test['precision']:.4f}\")\n",
    "    print(f\"Accuracy: {metrics_test['accuracy']:.4f}\")\n",
    "    print(f\"Specificity: {metrics_test['specificity']:.4f}\")\n",
    "    print(f\"Injury risk score(Recall): {metrics_test['injury_risk_score']:.4f}\")\n",
    "    print(f\"confusion_matrix: TN={metrics_test['confusion_matrix']['tn']}, FP={metrics_test['confusion_matrix']['fp']}, \"\n",
    "          f\"FN={metrics_test['confusion_matrix']['fn']}, TP={metrics_test['confusion_matrix']['tp']}\")\n",
    "\n",
    "    return metrics_test, metrics_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d72f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(evals_result, filename):\n",
    "    \"\"\"Plot the change of evaluation metrics during training\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Get the metric name from evaluation results\n",
    "    metric_name = list(evals_result['valid'].keys())[0]\n",
    "    \n",
    "    # Extract metric values during training\n",
    "    train_metric = evals_result['train'][metric_name]\n",
    "    valid_metric = evals_result['valid'][metric_name]\n",
    "    \n",
    "    # Plot metric changes for training and validation sets\n",
    "    plt.plot(range(1, len(train_metric) + 1), train_metric, 'b-', label='Training')\n",
    "    plt.plot(range(1, len(valid_metric) + 1), valid_metric, 'r-', label='Validation')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title(f'Learning Curve ({metric_name})')\n",
    "    plt.xlabel('Boosting Rounds')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_average_learning_curve(all_evals_results, filename):\n",
    "    \"\"\"Plot average learning curve across all folds\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Determine the maximum number of iterations across all folds\n",
    "    max_rounds = min(len(evals['valid']['auc']) for evals in all_evals_results)\n",
    "    \n",
    "    # Prepare arrays to store average values\n",
    "    avg_train = np.zeros(max_rounds)\n",
    "    avg_valid = np.zeros(max_rounds)\n",
    "    \n",
    "    # Collect metrics from all folds\n",
    "    train_metrics = []\n",
    "    valid_metrics = []\n",
    "    \n",
    "    for evals in all_evals_results:\n",
    "        train_auc = evals['train']['auc'][:max_rounds]\n",
    "        valid_auc = evals['valid']['auc'][:max_rounds]\n",
    "        train_metrics.append(train_auc)\n",
    "        valid_metrics.append(valid_auc)\n",
    "    \n",
    "    # Calculate average values\n",
    "    avg_train = np.mean(train_metrics, axis=0)\n",
    "    avg_valid = np.mean(valid_metrics, axis=0)\n",
    "    \n",
    "    # Plot the curves\n",
    "    plt.plot(range(1, max_rounds+1), avg_train, 'b-', label='Avg Training AUC')\n",
    "    plt.plot(range(1, max_rounds+1), avg_valid, 'r-', label='Avg Validation AUC')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title('Average Learning Curve (AUC)')\n",
    "    plt.xlabel('Boosting Rounds')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_multi_metric_curves(evals_result, filename_prefix):\n",
    "    \"\"\"Plot curves for multiple evaluation metrics\"\"\"\n",
    "    # Get all metric names\n",
    "    metric_names = list(evals_result['valid'].keys())\n",
    "    \n",
    "    for metric in metric_names:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Extract metric values during training\n",
    "        train_metric = evals_result['train'][metric]\n",
    "        valid_metric = evals_result['valid'][metric]\n",
    "        \n",
    "        # Plot metric changes for training and validation sets\n",
    "        plt.plot(range(1, len(train_metric) + 1), train_metric, 'b-', label=f'Training {metric}')\n",
    "        plt.plot(range(1, len(valid_metric) + 1), valid_metric, 'r-', label=f'Validation {metric}')\n",
    "        \n",
    "        # Add labels and title\n",
    "        plt.title(f'Learning Curve ({metric})')\n",
    "        plt.xlabel('Boosting Rounds')\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(f'{filename_prefix}_{metric}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "def plot_predicted_probability_distribution(y_true, y_prob, filename):\n",
    "    \"\"\"\n",
    "    Plot distribution of predicted probabilities grouped by true labels\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.hist([y_prob[y_true == 0], y_prob[y_true == 1]], \n",
    "             bins=50, stacked=True, \n",
    "             color=['skyblue', 'salmon'], \n",
    "             label=['Actual Negative', 'Actual Positive'])\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Predicted Probability Distribution by Actual Class')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confidence_accuracy_curve(y_true, y_prob, filename):\n",
    "    \"\"\"\n",
    "    Plot relationship between model confidence and accuracy\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays to avoid indexing issues\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    \n",
    "    # Sort by predicted probability\n",
    "    sorted_indices = np.argsort(y_prob)\n",
    "    sorted_prob = y_prob[sorted_indices]\n",
    "    sorted_true = y_true[sorted_indices]\n",
    "    \n",
    "    # Create probability bins\n",
    "    bins = np.linspace(0, 1, 21)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    accuracy = []\n",
    "    confidence = []\n",
    "    \n",
    "    # Calculate accuracy and confidence for each bin\n",
    "    for i in range(len(bins) - 1):\n",
    "        low, high = bins[i], bins[i+1]\n",
    "        mask = (sorted_prob >= low) & (sorted_prob < high)\n",
    "        if np.sum(mask) > 0:\n",
    "            bin_accuracy = np.mean(sorted_true[mask] == (sorted_prob[mask] > 0.5))\n",
    "            bin_confidence = np.mean(sorted_prob[mask])\n",
    "            accuracy.append(bin_accuracy)\n",
    "            confidence.append(bin_confidence)\n",
    "    \n",
    "    # Plot the relationship\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(confidence, accuracy, 'o-', color='blue', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    plt.xlabel('Model Confidence (Mean Predicted Probability)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Confidence-Accuracy Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_class_separation(X, y_true, y_prob, filename):\n",
    "    \"\"\"\n",
    "    Visualize class separation using dimensionality reduction\n",
    "    \"\"\"\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Scatter plot by true labels\n",
    "    plt.subplot(2, 2, 1)\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_true, cmap='coolwarm', alpha=0.6)\n",
    "    plt.colorbar(scatter, label='True Label')\n",
    "    plt.title('True Class Separation')\n",
    "    \n",
    "    # Scatter plot by predicted labels\n",
    "    plt.subplot(2, 2, 2)\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap='coolwarm', alpha=0.6)\n",
    "    plt.colorbar(scatter, label='Predicted Label')\n",
    "    plt.title('Predicted Class Separation')\n",
    "    \n",
    "    # Scatter plot by predicted probabilities\n",
    "    plt.subplot(2, 2, 3)\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_prob, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter, label='Predicted Probability')\n",
    "    plt.title('Predicted Probability Distribution')\n",
    "    \n",
    "    # Scatter plot highlighting misclassified points\n",
    "    plt.subplot(2, 2, 4)\n",
    "    misclassified = (y_true != y_pred)\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=misclassified, cmap='bwr', alpha=0.8)\n",
    "    plt.colorbar(scatter, ticks=[0, 1], label='Misclassified')\n",
    "    plt.title('Misclassification Points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9bc5759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(week_data_path=None, day_data_path=None, level=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess data\n",
    "    level: 'day' or 'week' level data\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    # If processed data does not exist, process data\n",
    "    # If processed data exists, load data\n",
    "    if  level=='day':\n",
    "        data = pd.read_csv(day_data_path) if day_data_path else None\n",
    "        processed_data = data_processing_pipeline(\n",
    "        week_data=data if level == 'week' else None,\n",
    "        day_data=data if level == 'day' else None,\n",
    "        level=level\n",
    "    )\n",
    "\n",
    "\n",
    "    else:\n",
    "        data = pd.read_csv(week_data_path) if week_data_path else None\n",
    "        processed_data = data_processing_pipeline(\n",
    "            week_data=data if level == 'week' else None,\n",
    "            day_data=data if level == 'day' else None,\n",
    "            level=level)\n",
    "\n",
    "\n",
    "    # Separate features and labels\n",
    "    X = processed_data.drop(columns=['injury', 'Athlete ID', 'Date'])\n",
    "    y = processed_data['injury']\n",
    "    \n",
    "    # Time-series split for train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y,  random_state=42\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd49a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PARAMS_FILE = \"results/lightgbm_best_params.json\"\n",
    "\n",
    "def save_best_params(best_params, filename=BEST_PARAMS_FILE):\n",
    "    \"\"\"Save best parameters to JSON file\"\"\"\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "    print(f\"Best parameters saved to {filename}\")\n",
    "\n",
    "def load_best_params(filename=BEST_PARAMS_FILE):\n",
    "    \"\"\"Load best parameters from JSON file\"\"\"\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"No best parameters file found at {filename}\")\n",
    "        return None\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    print(\"Loaded best parameters from file\")\n",
    "    return params\n",
    "\n",
    "def lightgbm_train_with_params(X_train, y_train, X_val=None, y_val=None, params=None):\n",
    "    \"\"\"Train LightGBM model with specified parameters\"\"\"\n",
    "\n",
    "    # Check class distribution\n",
    "    class_counts = Counter(y_train)\n",
    "    print(f\"Original class distribution: {dict(class_counts)}\")\n",
    "\n",
    "    # Apply sampling strategy\n",
    "    sampler = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "    X_res, y_res = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Print distribution after sampling\n",
    "    print(f\"Training set after sampling: Class distribution = {dict(Counter(y_res))}\")\n",
    "    print(f\"Resampled dataset size: {len(y_res)} (Positive: {sum(y_res)}, Negative: {len(y_res)-sum(y_res)})\")\n",
    "    \n",
    "    # Set default parameters\n",
    "    default_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': 9,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 300,\n",
    "        'min_child_samples': 20,\n",
    "        'reg_alpha': 0.01,\n",
    "        'reg_lambda': 0.01,\n",
    "        'scale_pos_weight': 2,  # Handle class imbalance\n",
    "        'class_weight': 'balanced',\n",
    "        'force_col_wise':True,\n",
    "        'verbose':-1\n",
    "    }\n",
    "    \n",
    "    # Update defaults if parameters are provided\n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = lgb.LGBMClassifier(**default_params)\n",
    "    # create a callback function to record evaluation results\n",
    "    evals_result = {}\n",
    "    callbacks = [record_evaluation(evals_result)]\n",
    "    \n",
    "    if X_val is not None and y_val is not None:\n",
    "        # Add early stopping\n",
    "        callbacks.append(early_stopping(stopping_rounds=50))\n",
    "        model.fit(\n",
    "            X_res, y_res,\n",
    "            eval_set=[(X_res, y_res), (X_val, y_val)],  # add validation set,\n",
    "            eval_names=['train', 'valid'],  # set evaluation names\n",
    "            eval_metric='auc',\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    else:\n",
    "        model.fit(X_res, y_res, callbacks=callbacks)\n",
    "    \n",
    "    return model, evals_result\n",
    "\n",
    "def train_model(X_train, y_train, use_best_params=False):\n",
    "    \"\"\"Train model (using time series cross-validation)\"\"\"\n",
    "    \n",
    "    # Try loading existing best parameters\n",
    "    best_params = load_best_params() if use_best_params else None\n",
    "    \n",
    "    # Check for valid data\n",
    "    if len(X_train) == 0 or len(y_train) == 0:\n",
    "        print(\"Error: Empty training data. Cannot train model.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Use time series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    models = []\n",
    "    scores = []\n",
    "    best_fold_models = []  # Store best model from each fold\n",
    "\n",
    "    all_evals_results = []  # store all eval results\n",
    "    \n",
    "    print(\"\\n=== Time Series Cross-Validation with LightGBM ===\")\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
    "        # Ensure validation set comes after training set (time-wise)\n",
    "        if min(val_idx) <= max(train_idx):\n",
    "            val_idx = val_idx[val_idx > max(train_idx)]\n",
    "            if len(val_idx) == 0:\n",
    "                print(f\"Skipping fold {fold_idx+1} - no valid validation data\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n--- Fold {fold_idx+1} ---\")\n",
    "        X_fold_train, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_fold_train, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        # Train LightGBM model (with early stopping)\n",
    "        print(\"Training LightGBM model...\")\n",
    "        # train lightgbm model and get eval results\n",
    "        lgb_model, evals_result = lightgbm_train_with_params(\n",
    "            X_fold_train, \n",
    "            y_fold_train, \n",
    "            X_val, \n",
    "            y_val,\n",
    "            params=best_params\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_val_prob = lgb_model.predict_proba(X_val)[:, 1]\n",
    "        lgb_auc = roc_auc_score(y_val, y_val_prob)\n",
    "        print(f\"LightGBM AUC: {lgb_auc:.4f}\")\n",
    "\n",
    "        # Save model and score\n",
    "        models.append(lgb_model)\n",
    "        scores.append(lgb_auc)\n",
    "        best_fold_models.append(lgb_model)\n",
    "\n",
    "        # save eval results\n",
    "        all_evals_results.append(evals_result)\n",
    "        \n",
    "        # plot learning curve of current fold\n",
    "        plot_learning_curve(evals_result, f'results/fold_{fold_idx+1}_learning_curve.png')\n",
    "        plot_multi_metric_curves(evals_result, f'results/fold_{fold_idx+1}')\n",
    "        \n",
    "    \n",
    "    # Select best model (based on validation AUC)\n",
    "    best_model_idx = np.argmax(scores)\n",
    "    best_model = models[best_model_idx]\n",
    "    print(f\"\\nBest model selected from fold {best_model_idx+1} with AUC: {scores[best_model_idx]:.4f}\")\n",
    "\n",
    "    plot_average_learning_curve(all_evals_results, 'results/average_learning_curve.png')\n",
    "    \n",
    "    # Save best parameters\n",
    "    if best_params is None:\n",
    "        save_best_params(best_model.get_params())\n",
    "    \n",
    "    return best_model, best_params, all_evals_results, best_model_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45063df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_model(model, X_train, X_test, y_train, y_test, model_name, evals_result=None):\n",
    "    \"\"\"Comprehensive evaluation of model performance\"\"\"\n",
    "\n",
    "    # Obtain predicted probabilities\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Determine optimal threshold\n",
    "    best_thresh = determine_best_threshold(y_train, y_train_prob, target_recall=0.75)\n",
    "    print(f\"Optimal threshold for recall: {best_thresh:.4f}\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics_test, metrics_val = evaluate_and_plot_model(\n",
    "        model_name,\n",
    "        y_train,\n",
    "        y_train_prob,\n",
    "        y_test,\n",
    "        y_test_prob,\n",
    "        best_thresh,\n",
    "        \"injury_model\"\n",
    "    )\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importances_  \n",
    "    })\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importances.to_csv('results/feature_importances.csv', index=False)\n",
    "\n",
    "    # Calculate the percentage importance\n",
    "    total_importance = feature_importances['importance'].sum()\n",
    "    feature_importances['pct_importance'] = (feature_importances['importance'] / total_importance * 100)\n",
    "    \n",
    "    # Sort by original importance (in descending order)\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importances.to_csv('results/feature_importances.csv', index=False)\n",
    "\n",
    "    if evals_result:\n",
    "        # 1. learning curve\n",
    "        plot_learning_curve(evals_result, 'results/final_model_learning_curve.png')\n",
    "    # 2. predicted_probability distribution\n",
    "    plot_predicted_probability_distribution(y_test, y_test_prob, 'results/predicted_prob_distribution.png')\n",
    "    # 3. confidence accuracy curve\n",
    "    plot_confidence_accuracy_curve(y_test, y_test_prob, 'results/confidence_accuracy_curve.png')\n",
    "    # 4. class separation\n",
    "    plot_class_separation(X_test, y_test, y_test_prob, 'results/class_separation.png')\n",
    "    \n",
    "    # Visualize top 20 features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    top_features = feature_importances.head(20).sort_values('pct_importance', ascending=True)\n",
    "    plt.barh(top_features['feature'], top_features['pct_importance'])\n",
    "    plt.xlabel('Importance (%)')\n",
    "    plt.title('Top 20 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/feature_importances.png', dpi=300)\n",
    "    plt.close()\n",
    "    return metrics_test, metrics_val, best_thresh, feature_importances\n",
    "\n",
    "def save_model_artifacts(model, metrics, threshold, filename):\n",
    "    \"\"\"Save model and related metadata\"\"\"\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, f\"{filename}.pkl\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'threshold': threshold,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    joblib.dump(metadata, f\"{filename}_metadata.pkl\")\n",
    "    \n",
    "    print(f\"Model and metadata saved to {filename}.pkl and {filename}_metadata.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92da848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "\n",
      "=== Model Training ===\n",
      "\n",
      "=== Time Series Cross-Validation with LightGBM ===\n",
      "\n",
      "--- Fold 1 ---\n",
      "Training LightGBM model...\n",
      "Original class distribution: {0: 5641, 1: 67}\n",
      "Training set after sampling: Class distribution = {0: 5641, 1: 2820}\n",
      "Resampled dataset size: 8461 (Positive: 2820, Negative: 5641)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttrain's auc: 0.99996\ttrain's binary_logloss: 0.0146026\tvalid's auc: 0.631906\tvalid's binary_logloss: 0.0840001\n",
      "LightGBM AUC: 0.6319\n",
      "\n",
      "--- Fold 2 ---\n",
      "Training LightGBM model...\n",
      "Original class distribution: {0: 11272, 1: 142}\n",
      "Training set after sampling: Class distribution = {0: 11272, 1: 5636}\n",
      "Resampled dataset size: 16908 (Positive: 5636, Negative: 11272)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttrain's auc: 0.994185\ttrain's binary_logloss: 0.327568\tvalid's auc: 0.698864\tvalid's binary_logloss: 0.368254\n",
      "LightGBM AUC: 0.6989\n",
      "\n",
      "--- Fold 3 ---\n",
      "Training LightGBM model...\n",
      "Original class distribution: {0: 16899, 1: 221}\n",
      "Training set after sampling: Class distribution = {0: 16899, 1: 8449}\n",
      "Resampled dataset size: 25348 (Positive: 8449, Negative: 16899)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttrain's auc: 0.981873\ttrain's binary_logloss: 0.438937\tvalid's auc: 0.706919\tvalid's binary_logloss: 0.474886\n",
      "LightGBM AUC: 0.7069\n",
      "\n",
      "--- Fold 4 ---\n",
      "Training LightGBM model...\n",
      "Original class distribution: {0: 22530, 1: 296}\n",
      "Training set after sampling: Class distribution = {0: 22530, 1: 11265}\n",
      "Resampled dataset size: 33795 (Positive: 11265, Negative: 22530)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttrain's auc: 0.987409\ttrain's binary_logloss: 0.32642\tvalid's auc: 0.69608\tvalid's binary_logloss: 0.370547\n",
      "LightGBM AUC: 0.6961\n",
      "\n",
      "--- Fold 5 ---\n",
      "Training LightGBM model...\n",
      "Original class distribution: {0: 28149, 1: 383}\n",
      "Training set after sampling: Class distribution = {0: 28149, 1: 14074}\n",
      "Resampled dataset size: 42223 (Positive: 14074, Negative: 28149)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttrain's auc: 0.981973\ttrain's binary_logloss: 0.423171\tvalid's auc: 0.756095\tvalid's binary_logloss: 0.465116\n",
      "LightGBM AUC: 0.7561\n",
      "\n",
      "Best model selected from fold 5 with AUC: 0.7561\n",
      "Best parameters saved to results/lightgbm_best_params.json\n",
      "\n",
      "=== Model Evaluation ===\n",
      "Optimal threshold for recall: 0.3535\n",
      "\n",
      "==== Injury Prediction Model Validation Set Performance ====\n",
      "AUC: 0.7762\n",
      "F1 Score: 0.0520\n",
      "Precision: 0.0269\n",
      "Accuracy: 0.6241\n",
      "Specificity: 0.6221\n",
      "Injury risk score(Recall): 0.7674\n",
      "confusion_matrix: TN=21014, FP=12764, FN=107, TP=353\n",
      "\n",
      "==== Injury Prediction Model Test Set Performance ====\n",
      "AUC: 0.7234\n",
      "F1 Score: 0.0460\n",
      "Precision: 0.0238\n",
      "Accuracy: 0.6217\n",
      "Specificity: 0.6210\n",
      "Injury risk score(Recall): 0.6783\n",
      "confusion_matrix: TN=5244, FP=3201, FN=37, TP=78\n",
      "\n",
      "=== Saving Results ===\n",
      "Model and metadata saved to results/injury_prediction_model.pkl and results/injury_prediction_model_metadata.pkl\n",
      "\n",
      "=== Final Model Performance ===\n",
      "AUC: 0.7234\n",
      "F1 Score: 0.0460\n",
      "Injury Risk Score (Recall): 0.6783\n",
      "Confusion Matrix:\n",
      "  TP: 78\n",
      "  FP: 3201\n",
      "  FN: 37\n",
      "  TN: 5244\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Configuration parameters\n",
    "    LEVEL = 'week'  # Switchbetween 'day' and 'week' level data\n",
    "    DATA_PATH_DAY = \"day_approach_maskedID_timeseries.csv\"  \n",
    "    DATA_PATH_WEEK = \"week_approach_maskedID_timeseries.csv\"  \n",
    "\n",
    "    # 1. Load and preprocess data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X_train, X_test, y_train, y_test, processed_data = load_and_preprocess_data(\n",
    "        day_data_path = DATA_PATH_DAY,\n",
    "        week_data_path = DATA_PATH_WEEK, \n",
    "        level=LEVEL\n",
    "    )\n",
    "\n",
    "\n",
    "    # 2. Train model (optionally using existing best parameters)\n",
    "    print(\"\\n=== Model Training ===\")\n",
    "    model, best_params, all_evals_results, best_model_idx = train_model(X_train, y_train)\n",
    "    \n",
    "    # 3. Evaluate model\n",
    "    print(\"\\n=== Model Evaluation ===\")\n",
    "    metrics_test, metrics_val, threshold, feature_importances = evaluate_final_model(\n",
    "        model, \n",
    "        X_train, \n",
    "        X_test, \n",
    "        y_train, \n",
    "        y_test,\n",
    "        \"Injury Prediction Model\",\n",
    "        evals_result=all_evals_results[best_model_idx]\n",
    "    )\n",
    "    \n",
    "    # 4. Save model and results\n",
    "    print(\"\\n=== Saving Results ===\")\n",
    "    save_model_artifacts(model, metrics_test, threshold, \"results/injury_prediction_model\")\n",
    "    \n",
    "    # 5. Final performance report\n",
    "    print(\"\\n=== Final Model Performance ===\")\n",
    "    print(f\"AUC: {metrics_test['auc']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics_test['f1']:.4f}\")\n",
    "    print(f\"Injury Risk Score (Recall): {metrics_test['injury_risk_score']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(f\"  TP: {metrics_test['confusion_matrix']['tp']}\")\n",
    "    print(f\"  FP: {metrics_test['confusion_matrix']['fp']}\")\n",
    "    print(f\"  FN: {metrics_test['confusion_matrix']['fn']}\")\n",
    "    print(f\"  TN: {metrics_test['confusion_matrix']['tn']}\")\n",
    "    \n",
    "    # 6. Save best parameters (if needed)\n",
    "    if best_params:\n",
    "        save_best_params(best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
